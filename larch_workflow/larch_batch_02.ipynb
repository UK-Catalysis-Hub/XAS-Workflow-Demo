{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAS Workflow Task 2\n",
    "\n",
    "This notebook contains the second task of the XAS processing workflow. \n",
    "\n",
    "The break up of the task consist of the following steps \n",
    "\n",
    "|Task                            | Input                                         | Output\n",
    "|-------------                   |-------------                                  |-----  \n",
    "| Curve fitting||\n",
    "| 1. Import data                |File: FeS2_larch.prj                              |\n",
    "| 2. Import Crystal data        |File: FeS2.inp                                 |\n",
    "| 3. Calculate Paths(Atoms+FEFF)||\n",
    "| 4. Set path parameters        | Parameters:                                   |\n",
    "|                                 |    amp  = 1                                   |\n",
    "|                                 |    enot = 0                                   |\n",
    "|                                 |    delr = 0                                   |\n",
    "|                                 |    ss   = 0.003                               |\n",
    "| 5. Select paths                 |                                               |\n",
    "| 6. Run Fit                    |                                               |\n",
    "| 7. Save project               ||\n",
    "| 8. Verify fit results         ||\n",
    "| 8.1 If not OK revise parameners and refit (go to 2.4)||\n",
    "| 8.2 If OK Save project and outputs|                                           |File: FeS2_01.fpj\n",
    "\n",
    "For more details about larch, see https://xraypy.github.io/xraylarch/xafs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "Libraries required for running fit on the XAS data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# managing athena files\n",
    "from larch.io import create_athena, read_athena, extract_athenagroup\n",
    "# calculate pre-edge and post edge for normalisation\n",
    "from larch.xafs import pre_edge\n",
    "# perform background removal\n",
    "from larch.xafs import autobk\n",
    "# calculate fourier transform\n",
    "from larch.xafs import xftf\n",
    "\n",
    "\n",
    "\n",
    "from larch import Interpreter\n",
    "#import larch_plugins as lp\n",
    "\n",
    "# File handling\n",
    "from pathlib import Path\n",
    "\n",
    "#plotting library\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# subprocess library used to run perl script\n",
    "import subprocess\n",
    "\n",
    "#library for writing to log\n",
    "import logging\n",
    "\n",
    "\n",
    "# Library with the functions that handle athena files\n",
    "import lib.manage_athena as athenamgr  \n",
    "\n",
    "# Library with the functions that execute \n",
    "# Atoms and FEFF to generate scattering paths\n",
    "import lib.atoms_feff as feff_runner   \n",
    "\n",
    "# Set parameters          \n",
    "# library containign functions tho manage fit, at read, write \n",
    "# GDS parameters, and scattering paths. \n",
    "import lib.manage_fit as fit_manager  \n",
    "\n",
    "\n",
    "\n",
    "session = Interpreter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Functions\n",
    "\n",
    "The following cell contains the defined functions (methods) for processing XAS files.\n",
    "\n",
    "- **set_logger** intialises the logging.\n",
    "- **get_files_list** returns a list of files in the directory matching the given file pattern.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " #######################################################\n",
    "# |                Initialise log file                | #\n",
    "# V  provide the path and name of the log file to use V #\n",
    " #######################################################\n",
    "    \n",
    "def set_logger(log_file):\n",
    "    logger = logging.getLogger()\n",
    "    fhandler = logging.FileHandler(filename=log_file, mode='a')\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fhandler.setFormatter(formatter)\n",
    "    logger.addHandler(fhandler)\n",
    "    # prevent matplotlib font manager from writing to log\n",
    "    logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    " #######################################################\n",
    "# |                  Get a list of files              | #\n",
    "# V       provide the path and pattern to match       V #\n",
    " #######################################################\n",
    "    \n",
    "#reading all files with the same extension files from a dir\n",
    "def get_files_list(source_dir, f_pattern):\n",
    "    i_counter = 0\n",
    "    files_list = []\n",
    "    for filepath in sorted(source_dir.glob(f_pattern)):\n",
    "        i_counter += 1\n",
    "        files_list.append(filepath)\n",
    "    return files_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input parameters (variables)\n",
    "\n",
    "The variables in the next cell indicate where to get the data from, the pattern of the files to process and the number of files to process. These can be changed to process different datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables that can be changed to process different datasets\n",
    "data_path = \".\\\\rh4co\\\\\"\n",
    "file_pattern = \"*.prj\"\n",
    "f_prefix = \"rh4co\"\n",
    "crystal_files = [\"..\\\\cif_files\\\\C12O12Rh4.cif\"] #\"FeS2.inp\"\n",
    "gds_parms_f = \"rh4co40_gds.csv\"\n",
    "sel_paths_f = \"rh4co40_sp.csv\"\n",
    "top_count = 500 # limit the number of files processed \n",
    "show_graph = False # False to prevent showing graphs\n",
    "\n",
    "# variables for fit\n",
    "fit_vars = {}\n",
    "fit_vars['fitspace']='r'\n",
    "fit_vars['kmin']=3 \n",
    "fit_vars['kmax']=14\n",
    "fit_vars['kw']=2 \n",
    "fit_vars['dk']=1\n",
    "fit_vars['window']='hanning'\n",
    "fit_vars['rmin']=1.4\n",
    "fit_vars['rmax']=3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read save parameters from input gds file\n",
    "gds = fit_manager.read_gds(gds_parms_f, session)\n",
    "# show gsd group parameters in a spreadsheet\n",
    "this_sheet = fit_manager.show_gds(gds)\n",
    "# save gsd group parameters in a csv file\n",
    "fit_manager.save_gds(gds, gds_parms_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function\n",
    "The code in the cell below performs the bulk of the processing for task 2 calling functions defined above and using the input parameters.\n",
    "\n",
    "The three operations for generating paths, selecting paths and defining parameters are performed once before processing the input files in the list\n",
    "\n",
    "### Generate paths \n",
    "Larch does larch does not include a means for running **Atoms**. Atoms is needed to get input for **FEFF** and calculate paths. Currently, the fastest option is to run Artemis to obtain the input (.inp) file for FEFF from a crystal file ('.cif' or '.inp').\n",
    "\n",
    "The run_feff function, defined above, uses the subprocess library to call perl to execute a script that runs Artemis Atoms, and saves the output file ('.inp') in a new directory. The file is then be used to run FEFF from Larch to calculate scattering paths.\n",
    "\n",
    "### Select paths and set parameters\n",
    "Select paths and set parameters are tasks which can be performed for the first fit  and then reused for all following datasets.  \n",
    "\n",
    "#### Set Parameters\n",
    "\n",
    "The read_gds function, defined above, creates the parameters group (GDS in Artemis) from a gds file. The code uses lp.fitting.param_group and lp.fitting.param instead of importing Group and Parameter (does not work with Jupyter). The parameter values are read from a text file.\n",
    "\n",
    "#### Select Paths\n",
    "The selection of scattering paths is handled by the read_paths function. Each scattering path is loaded while setting the amplitude, $\\Delta E_0$, $\\Delta R$ and $\\sigma^2$ parameters using the GDS parameters defined previously. The groups are then added returned as a list of paths to be used for the fits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the path for storing results\n",
    "base_path = Path(\"./\" , f_prefix+\"_fit\")\n",
    "Path(base_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_file = Path(\"./\",base_path,\"process.log\")\n",
    "print(log_file)\n",
    "# set path for log\n",
    "set_logger(log_file)\n",
    "\n",
    "# get the list of files to process\n",
    "source_path = Path(data_path)\n",
    "files_list = get_files_list(source_path, file_pattern)\n",
    "xas_data = {}\n",
    "\n",
    "logging.info(\"Started processing\")\n",
    "logging.info(\"Input variables:\")\n",
    "logging.info(\"\\tdata_path    = \" + data_path)\n",
    "logging.info(\"\\tfile_pattern = \" + file_pattern)\n",
    "logging.info(\"\\tf_prefix     = \" + f_prefix)\n",
    "logging.info(\"\\tcrystal_files = \" + str(crystal_files))\n",
    "logging.info(\"\\ttop_count    = \" + str(top_count))\n",
    "\n",
    "\n",
    "\n",
    "# run feff on crystal file to generate scattering paths\n",
    "feff_runner.run_feff(crystal_files)\n",
    "logging.info(\"Completed FEFF\")\n",
    "\n",
    "\n",
    "# counter for break\n",
    "i_count = 0\n",
    "for a_file in files_list:\n",
    "    \n",
    "    # read the gds parameters from input file\n",
    "    gds = fit_manager.read_gds(gds_parms_f, session)\n",
    "    logging.info(\"GDS Parameters read OK\")\n",
    "    project_name = a_file.name\n",
    "    data_prj = read_athena(a_file)\n",
    "    group_keys = list(data_prj._athena_groups.keys())\n",
    "    athena_group = extract_athenagroup(data_prj._athena_groups[group_keys[0]])\n",
    "    # recalculate norm, background removal and fourier transform \n",
    "    # with defaults\n",
    "    data_group = athenamgr.calc_with_defaults(athena_group)\n",
    "\n",
    "    # read the selected paths list to access relevant paths \n",
    "    # generated from FEFF\n",
    "    selected_paths = fit_manager.read_selected_paths_list(sel_paths_f, session)\n",
    "    logging.info(\"Selected Paths read OK\")\n",
    "    # run fit\n",
    "    trans, dset, out = fit_manager.run_fit(data_group, gds, selected_paths, fit_vars, session)\n",
    "    \n",
    "    \n",
    "    if show_graph:    \n",
    "        # plot normalised mu on energy\n",
    "        # plot mu vs flat normalised mu for selected groups\n",
    "        #plt, data_group = plot_Nxmu_E(data_prj, group_keys, group_names)\n",
    "        plt = athenamgr.plot_normalised(data_group)\n",
    "        plt.show()\n",
    "        # Overlaped 𝜒(𝑘) and 𝜒(𝑅) plots (similar to Demeter's Rmr plot)\n",
    "        \n",
    "        rmr_p = fit_manager.plot_rmr(dset,fit_vars['rmin'],fit_vars['rmax'])\n",
    "        rmr_p.show()\n",
    "        \n",
    "        # Separate 𝜒(𝑘) and 𝜒(𝑅) plots \n",
    "        chikr_p = fit_manager.plot_chikr(dset,fit_vars['rmin'],fit_vars['rmax'],fit_vars['kmin'],fit_vars['kmax'])\n",
    "        chikr_p.show()\n",
    "    \n",
    "\n",
    "    #save the fit report to a text file\n",
    "    fit_file = Path(\"./\",base_path,group_keys[0]+\"_fit_rep.txt\")\n",
    "    fit_manager.save_fit_report(out, fit_file, session)\n",
    "\n",
    "    i_count +=1\n",
    "    \n",
    "    logging.info(\"Processed file: \"+ str(i_count) +\" \" + group_keys[0])\n",
    "    \n",
    "\n",
    "    \n",
    "    if i_count == top_count:\n",
    "        break\n",
    "       \n",
    "logging.info(\"Finished processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to do\n",
    "- for manual task\n",
    " - ~Allow setting parameters interactively (if CSV file does not exist or is missing)~\n",
    " - ~Move all functions to libraries (hide complexity from researchers)~\n",
    " - ~Test running with textbook example (FeS2)~\n",
    " - ~Test running with sample data (Rh4CO12)~\n",
    "- for batch processing\n",
    " - ~replicate in batch processing (reading of parameters and paths)~\n",
    " - ~test run automatic with textbook example~\n",
    " - test run automatic with sample data (Rh4CO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
