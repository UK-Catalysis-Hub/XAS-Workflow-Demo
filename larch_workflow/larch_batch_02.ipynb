{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAS Workflow Task 2\n",
    "\n",
    "This notebook contains the second task of the XAS processing workflow. \n",
    "\n",
    "The break up of the task consist of the following steps \n",
    "\n",
    "|Task                            | Input                                         | Output\n",
    "|-------------                   |-------------                                  |-----  \n",
    "| Curve fitting||\n",
    "| 1. Import data                |File: FeS2_larch.prj                              |\n",
    "| 2. Import Crystal data        |File: FeS2.inp                                 |\n",
    "| 3. Calculate Paths(Atoms+FEFF)||\n",
    "| 4. Set path parameters        | Parameters:                                   |\n",
    "|                                 |    amp  = 1                                   |\n",
    "|                                 |    enot = 0                                   |\n",
    "|                                 |    delr = 0                                   |\n",
    "|                                 |    ss   = 0.003                               |\n",
    "| 5. Select paths                 |                                               |\n",
    "| 6. Run Fit                    |                                               |\n",
    "| 7. Save project               ||\n",
    "| 8. Verify fit results         ||\n",
    "| 8.1 If not OK revise parameners and refit (go to 2.4)||\n",
    "| 8.2 If OK Save project and outputs|                                           |File: FeS2_01.fpj\n",
    "\n",
    "For more details about larch, see https://xraypy.github.io/xraylarch/xafs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "Libraries required for running fit on the XAS data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# managing athena files\n",
    "from larch.io import create_athena, read_athena, extract_athenagroup\n",
    "# calculate pre-edge and post edge for normalisation\n",
    "from larch.xafs import pre_edge\n",
    "# perform background removal\n",
    "from larch.xafs import autobk\n",
    "# calculate fourier transform\n",
    "from larch.xafs import xftf\n",
    "\n",
    "from larch import Interpreter\n",
    "import larch_plugins as lp\n",
    "\n",
    "# File handling\n",
    "from pathlib import Path\n",
    "\n",
    "#plotting library\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# subprocess library used to run perl script\n",
    "import subprocess\n",
    "\n",
    "#library for writing to log\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Functions\n",
    "\n",
    "The following cell contains the defined functions (methods) for processing XAS files.\n",
    "\n",
    "- **set_logger** intialises the logging.\n",
    "- **get_files_list** returns a list of files in the directory matching the given file pattern.\n",
    "- **calc_with_defaults ** recalculates mu, normal, pre-edge for each group being processed.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " #######################################################\n",
    "# |                Initialise log file                | #\n",
    "# V  provide the path and name of the log file to use V #\n",
    " #######################################################\n",
    "    \n",
    "def set_logger(log_file):\n",
    "    logger = logging.getLogger()\n",
    "    fhandler = logging.FileHandler(filename=log_file, mode='a')\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fhandler.setFormatter(formatter)\n",
    "    logger.addHandler(fhandler)\n",
    "    # prevent matplotlib font manager from writing to log\n",
    "    logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    " #######################################################\n",
    "# |                  Get a list of files              | #\n",
    "# V       provide the path and pattern to match       V #\n",
    " #######################################################\n",
    "    \n",
    "#reading all files with the same extension files from a dir\n",
    "def get_files_list(source_dir, f_pattern):\n",
    "    i_counter = 0\n",
    "    files_list = []\n",
    "    for filepath in sorted(source_dir.glob(f_pattern)):\n",
    "        i_counter += 1\n",
    "        files_list.append(filepath)\n",
    "    return files_list\n",
    "\n",
    " #######################################################\n",
    "# |         Athena recalculates everything so we      | #\n",
    "# |      need to create a function that calculates    | #\n",
    "# V               all for each new group              V #\n",
    " #######################################################\n",
    "\n",
    "def calc_with_defaults(xafs_group):\n",
    "    # calculate mu and normalise with background extraction\n",
    "    # should let the user specify the colums for i0, it, mu, iR. \n",
    "    if not hasattr(xafs_group, 'mu'):\n",
    "        xafs_group = get_mu(xafs_group)\n",
    "    # calculate pre-edge and post edge and add them to group\n",
    "    # need to read parameters for pre-edge before background calculation with  \n",
    "    # defaul values undo the work of previous step (setting pre-edge limits).\n",
    "    pre_edge(xafs_group, pre1=xafs_group.bkg_params.pre1, pre2=xafs_group.bkg_params.pre2)\n",
    "    #pre_edge(xafs_group)\n",
    "    # perform background removal\n",
    "    autobk(xafs_group) # using defaults so no additional parameters are passed\n",
    "    # calculate fourier transform\n",
    "    xftf(xafs_group)#, kweight=0.5, kmin=3.0, kmax=12.871, dk=1, kwindow='Hanning')\n",
    "    return xafs_group\n",
    "\n",
    "\n",
    " #######################################################\n",
    "# |       The code for plotting Nmu vs E repeats      | #\n",
    "# |   so it is useful to have a plotting function     | #\n",
    "# V            to reduce duplicated code              V #\n",
    " #######################################################\n",
    "# plot flat normalised mu vs Energy for selected groups\n",
    "def plot_Nxmu_E(athena_project, group_keys, group_names,\n",
    "                            title = \"Normalised $\\mu$ vs E\", xlimits = None,\n",
    "                            ylimits = None):    \n",
    "    # plot mu vs flat normalised mu for selected groups\n",
    "    for group_key in group_keys:\n",
    "        gr_0 = extract_athenagroup(athena_project._athena_groups[group_key])\n",
    "        # recalculate normalisation\n",
    "        calc_with_defaults(gr_0)\n",
    "        plt.plot(gr_0.energy, gr_0.flat, label=group_names[group_key])\n",
    "\n",
    "    # set plot format\n",
    "    plt.xlabel(\"Energy\")\n",
    "    plt.ylabel(\"Normalised $\\mu$\" )\n",
    "    plt.title(title)\n",
    "    plt.grid(linestyle=':', linewidth=1) #show and format grid\n",
    "    if xlimits != None:\n",
    "        plt.xlim(xlimits[0],xlimits[1])\n",
    "    if ylimits != None:\n",
    "        plt.ylim(ylimits[0],ylimits[1])\n",
    "    plt.legend()\n",
    "    return plt, gr_0\n",
    "\n",
    " ########################################################\n",
    "# |                Get scattering paths                | #\n",
    "# | larch does not include a means for running atoms   | #\n",
    "# | need to get input for feff and calculate paths     | #\n",
    "# | currently the fastest option is to run Artemis to  | #\n",
    "# | obtain the input (.inp) file for feff from a '.cif'| #\n",
    "# V or '.inp' file                                     V #\n",
    " ########################################################\n",
    "def run_feff(var = \"FeS2.inp\"):\n",
    "    crystal_f = Path(var)\n",
    "    feff_dir = crystal_f.name[:-4]+\"_feff\"\n",
    "    feff_inp = crystal_f.name[:-4]+\"_feff.inp\"\n",
    "    retcode = subprocess.call([\"perl\", \"feff_inp.pl\", var, feff_dir, feff_inp])\n",
    "    if retcode == 0:\n",
    "        print(\"Passed!\")\n",
    "    else:\n",
    "        print(\"Failed!\")\n",
    "    \n",
    "    # run feff and get the paths\n",
    "    from larch.xafs.feffrunner import feff6l\n",
    "    #feff6l(folder='.', feffinp='feff.inp', verbose=True)\n",
    "    feff6l(folder = feff_dir, feffinp=feff_inp)\n",
    " ########################################################\n",
    "# |                Read GDS parameters                 | #\n",
    "# | read parameters from gds file                      | #\n",
    "# | each line contains a parameter defined as          | #\n",
    "# V type name = value                                  V #\n",
    " ########################################################\n",
    "# guess amp = 1.00000\n",
    "# guess enot = 0\n",
    "# skip delr = 0\n",
    "# guess ss = 0.00300\n",
    "# guess alpha = 0\n",
    "# guess ss2 = 0.00300\n",
    "# guess ss3 = ss2\n",
    "# guess ssFe = 0.00300\n",
    "# skip ss4 = 0.00300\n",
    "##############################\n",
    "def read_gds(gds_file = \"FeS2_dmtr.gds\"):\n",
    "    session = Interpreter()\n",
    "    dgs_group = lp.fitting.param_group(_larch=session)\n",
    "    with open(gds_file, 'r') as reader:\n",
    "        # Read and print the entire file line by line\n",
    "        gds_par = reader.readline()\n",
    "        while gds_par != '':  # The EOF char is an empty string\n",
    "            gds_par_def = gds_par.split()\n",
    "            #gds file structure:\n",
    "            # type name = value\n",
    "            gds_type = gds_par_def[0]\n",
    "            gds_name = gds_par_def[1]\n",
    "            gds_val = 0.0\n",
    "            gds_exp = \"\"\n",
    "            #print(gds_par_def, \"\\n\", end='')\n",
    "            try:\n",
    "                gds_val = float(gds_par_def[3])\n",
    "            except ValueError:\n",
    "                #print(\"Not a float value\")\n",
    "                gds_val = 0.00\n",
    "                gds_exp = gds_par_def[3]\n",
    "            one_par = None\n",
    "            if gds_type == \"guess\":\n",
    "                one_par = lp.fitting.guess(name=gds_name ,value=gds_val, vary=True, expr=gds_exp )\n",
    "            elif gds_type == \"def\":\n",
    "                one_par = lp.fitting.param(name=gds_name ,value=gds_val, vary=False, expr=gds_exp )\n",
    "            if one_par != None:\n",
    "                 dgs_group.__setattr__(gds_par_def[1],one_par )\n",
    "            gds_par = reader.readline()\n",
    "    return dgs_group\n",
    "      \n",
    "# show plot of normalised data\n",
    "def plot_normalised(xafs_group):\n",
    "        plt.plot(xafs_group.energy, xafs_group.mu, label=xafs_group.filename) # plot mu in blue\n",
    "        plt.grid(color='r', linestyle=':', linewidth=1) #show and format grid\n",
    "        plt.xlabel('Energy (eV)') # label y graph\n",
    "        plt.ylabel('x$\\mu$(E)') # label y axis\n",
    "        plt.title(\"pre-edge and post_edge fitting to $\\mu$\")\n",
    "        plt.legend() # show legend\n",
    "        return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input parameters (variables)\n",
    "\n",
    "The variables in the next cell indicate where to get the data from, the pattern of the files to process and the number of files to process. These can be changed to process different datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables that can be changed to process different datasets\n",
    "data_path = \".\\\\rh4co\\\\\"\n",
    "file_pattern = \"*.prj\"\n",
    "f_prefix = \"rh4co\"\n",
    "crystal_files = [\"..\\\\cif_files\\\\C12O12Rh4.cif\"] #\"FeS2.inp\"\n",
    "gds_parms_f = \"rh4co.gds\"\n",
    "sel_paths_f = \"rh4co.csv\"\n",
    "top_count = 400\n",
    "show_graph = False # False to prevent showing graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function\n",
    "The code in the cell below performs the bulk of the processing for task 2 calling functions defined above and using the input parameters.\n",
    "\n",
    "The three operations for generating paths, selecting paths and defining parameters are performed once before processing the input files in the list\n",
    "\n",
    "### Generate paths \n",
    "Larch does larch does not include a means for running **Atoms**. Atoms is needed to get input for **FEFF** and calculate paths. Currently, the fastest option is to run Artemis to obtain the input (.inp) file for FEFF from a crystal file ('.cif' or '.inp').\n",
    "\n",
    "The run_feff function, defined above, uses the subprocess library to call perl to execute a script that runs Artemis Atoms, and saves the output file ('.inp') in a new directory. The file is then be used to run FEFF from Larch to calculate scattering paths.\n",
    "\n",
    "### Select paths and set parameters\n",
    "Select paths and set parameters are tasks which can be performed for the first fit  and then reused for all following datasets.  \n",
    "\n",
    "#### Set Parameters\n",
    "\n",
    "The read_gds function, defined above, creates the parameters group (GDS in Artemis) from a gds file. The code uses lp.fitting.param_group and lp.fitting.param instead of importing Group and Parameter (does not work with Jupyter). The parameter values are read from a text file.\n",
    "\n",
    "#### Select Paths\n",
    "The selection of scattering paths is handled by the read_paths function. Each scattering path is loaded while setting the amplitude, $\\Delta E_0$, $\\Delta R$ and $\\sigma^2$ parameters using the GDS parameters defined previously. The groups are then added returned as a list of paths to be used for the fits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root] INFO : Started processing\n",
      "[root] INFO : Input variables:\n",
      "[root] INFO : \tdata_path    = .\\rh4co\\\n",
      "[root] INFO : \tfile_pattern = *.prj\n",
      "[root] INFO : \tf_prefix     = rh4co\n",
      "[root] INFO : \tcrystal_file = ..\\cif_files\\C12O12Rh4.cif\n",
      "[root] INFO : \ttop_count    = 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rh4co_fit\\process.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root] INFO : GDS Parameters read OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed!\n",
      " : ======== running Feff module C:\\Users\\scman1\\Anaconda3\\envs\\python36\\lib\\site-packages\\larch\\bin\\win32\\feff6l.exe ========\n",
      " : Feff 6L.02\n",
      " : Rh4 (C O)12\n",
      " : Wei, C.H.\n",
      " : Structural analyses of tetracobalt dodecacarbonyl and tetrarhodium\n",
      " : dodecacarbonyl. crystallographic treatments of a disordered structure and\n",
      " : a twinned composite\n",
      " : Calculating potentials and phases...\n",
      " : free atom potential and density for atom type    0\n",
      " : free atom potential and density for atom type    1\n",
      " : free atom potential and density for atom type    2\n",
      " : free atom potential and density for atom type    3\n",
      " : overlapped potential and density for unique potential    0\n",
      " : overlapped potential and density for unique potential    1\n",
      " : overlapped potential and density for unique potential    2\n",
      " : overlapped potential and density for unique potential    3\n",
      " : muffin tin radii and interstitial parameters\n",
      " : phase shifts for unique potential    0\n",
      " : phase shifts for unique potential    1\n",
      " : phase shifts for unique potential    2\n",
      " : phase shifts for unique potential    3\n",
      " : Preparing plane wave scattering amplitudes...\n",
      " : nncrit in prcrit       9\n",
      " : Searching for paths...\n",
      " : Rmax  5.0000  keep and heap limits   0.0000000   0.0000000\n",
      " : Preparing neighbor table\n",
      " : nfound  nheap  nheapx  nsc    r\n",
      " : Paths found      403   (nheapx, nbx      99   8)\n",
      " : Eliminating path degeneracies...\n",
      " : Plane wave chi amplitude filter   2.50%\n",
      " : Unique paths    115,  total paths     177\n",
      " : Calculating EXAFS parameters...\n",
      " : Curved wave chi amplitude ratio   4.00%\n",
      " : Discard feff.dat for paths with cw ratio <   2.67%\n",
      " : path  cw ratio     deg    nleg  reff\n",
      " : 1   100.000     1.000     2   1.9149\n",
      " : 2    91.295     1.000     2   1.9854\n",
      " : 3    83.637     1.000     2   2.0550\n",
      " : 4    79.929     1.000     2   2.0917\n",
      " : 5    89.367     1.000     2   2.7050\n",
      " : 6    86.941     1.000     2   2.7337\n",
      " : 7    81.971     1.000     2   2.7957\n",
      " : 8    37.159     1.000     2   3.0163\n",
      " : 9   100.000     2.000     3   3.0167\n",
      " : 10    76.145     1.000     4   3.0170\n",
      " : 11    32.597     1.000     2   3.0220\n",
      " : 12    29.179     1.000     2   3.1366\n",
      " : 13    25.921     2.000     3   3.1653\n",
      " : 14    27.950     1.000     2   3.1819\n",
      " : 15    58.926     2.000     3   3.2132\n",
      " : 16    32.939     1.000     4   3.2445\n",
      " : 17    21.901     2.000     3   3.2875\n",
      " : 18     7.728     1.000     4   3.3087\n",
      " : 19     5.677     2.000     3   3.3214\n",
      " : 20     5.763     2.000     3   3.3260\n",
      " : 21     6.231     1.000     4   3.4384\n",
      " : 22     2.425     2.000     3   3.4558 neglected\n",
      " : 23    16.232     1.000     2   3.5372\n",
      " : 24     3.728     2.000     3   3.5425\n",
      " : 25    15.819     1.000     2   3.5671\n",
      " : 26     3.312     2.000     3   3.5939\n",
      " : 27     4.223     2.000     3   3.6181\n",
      " : 28     2.572     2.000     3   3.6489 neglected\n",
      " : 29    14.227     1.000     2   3.6920\n",
      " : 30    11.517     2.000     3   3.8220\n",
      " : 31     4.455     1.000     4   3.8297\n",
      " : 32    12.091     1.000     2   3.8888\n",
      " : 33    15.264     2.000     4   3.9003\n",
      " : 34    14.184     1.000     2   3.9535\n",
      " : 35     3.684     1.000     4   3.9708\n",
      " : 36    10.240     1.000     4   3.9783\n",
      " : 37    32.052     2.000     5   3.9787\n",
      " : 38    25.210     1.000     6   3.9790\n",
      " : 39    11.124     1.000     2   3.9920\n",
      " : 40    13.715     1.000     2   3.9945\n",
      " : 41     8.332     2.000     3   4.0838\n",
      " : 42     3.048     2.000     3   4.0890\n",
      " : 43     3.260     2.000     3   4.1013\n",
      " : 44     3.073     1.000     4   4.1100\n",
      " : 45     7.914     2.000     3   4.1209\n",
      " : 46     3.206     2.000     3   4.1223\n",
      " : 47     2.799     1.000     4   4.1834\n",
      " : 48     7.874     2.000     3   4.1912\n",
      " : 49     1.425     2.000     3   4.2145 neglected\n",
      " : 50     8.949     1.000     2   4.2696\n",
      " : 51    10.748     1.000     2   4.3002\n",
      " : 52    10.714     1.000     2   4.3043\n",
      " : 53    10.621     1.000     2   4.3155\n",
      " : 54     7.182     2.000     3   4.3251\n",
      " : 55     5.235     1.000     4   4.3347\n",
      " : 56     1.577     2.000     3   4.3415 neglected\n",
      " : 57     3.558     1.000     4   4.3452\n",
      " : 58     8.350     1.000     2   4.3603\n",
      " : 59    12.514     2.000     5   4.3660\n",
      " : 60    16.294     2.000     3   4.3787\n",
      " : 61     1.866     2.000     3   4.3964 neglected\n",
      " : 62     7.648     1.000     6   4.3972\n",
      " : 63     6.643     2.000     3   4.4001\n",
      " : 64    13.337     2.000     3   4.4055\n",
      " : 65     2.179     2.000     4   4.4289 neglected\n",
      " : 66     5.926     2.000     3   4.4541\n",
      " : 67     1.929     2.000     4   4.4805 neglected\n",
      " : 68     9.336     1.000     2   4.4832\n",
      " : 69     7.222     1.000     4   4.4878\n",
      " : 70     4.653     2.000     5   4.4886\n",
      " : 71    12.751     2.000     3   4.4927\n",
      " : 72     5.932     1.000     4   4.4955\n",
      " : 73     2.547     2.000     5   4.5045 neglected\n",
      " : 74     2.232     2.000     4   4.5397 neglected\n",
      " : 75     2.262     2.000     5   4.5559 neglected\n",
      " : 76     1.888     2.000     3   4.5571 neglected\n",
      " : 77     1.544     2.000     3   4.5599 neglected\n",
      " : 78     3.284     2.000     3   4.5615\n",
      " : 79     2.902     2.000     3   4.5732\n",
      " : 80     3.436     2.000     3   4.6064\n",
      " : 81     4.653     1.000     4   4.6251\n",
      " : 82     6.854     2.000     3   4.6311\n",
      " : 83     6.727     1.000     2   4.6507\n",
      " : 84     8.204     1.000     2   4.6551\n",
      " : 85     2.780     2.000     3   4.6567\n",
      " : 86     2.354     1.000     4   4.6601 neglected\n",
      " : 87     3.264     2.000     3   4.6645\n",
      " : 88     6.619     1.000     2   4.6730\n",
      " : 89     3.068     2.000     4   4.6823\n",
      " : 90     1.803     2.000     4   4.7108 neglected\n",
      " : 91    18.587     2.000     3   4.7211\n",
      " : 92    13.756     1.000     4   4.7692\n",
      " : 93     2.182     1.000     4   4.7791 neglected\n",
      " : 94     6.951     2.000     3   4.7831\n",
      " : 95     6.102     2.000     3   4.7842\n",
      " : 96     2.095     2.000     4   4.7887 neglected\n",
      " : 97     2.457     2.000     4   4.7891 neglected\n",
      " : 98    10.393     2.000     4   4.7967\n",
      " : 99     2.759     2.000     5   4.8110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root] INFO : Completed FEFF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " : 100     0.984     2.000     3   4.8201 neglected\n",
      " : 101     1.949     2.000     4   4.8507 neglected\n",
      " : 102     7.049     1.000     2   4.8622\n",
      " : 103     2.435     2.000     3   4.8868 neglected\n",
      " : 104     6.792     1.000     2   4.9136\n",
      " : 105     5.690     2.000     3   4.9161\n",
      " : 106     2.369     1.000     4   4.9155 neglected\n",
      " : 107    11.074     2.000     3   4.9167\n",
      " : 108    10.137     2.000     4   4.9368\n",
      " : 109     5.170     1.000     6   4.9403\n",
      " : 110    16.546     2.000     7   4.9407\n",
      " : 111    13.280     1.000     8   4.9410\n",
      " : 112     3.562     2.000     3   4.9549\n",
      " : 113     5.689     2.000     4   4.9593\n",
      " : 114     5.096     1.000     4   4.9712\n",
      " : 115     6.492     1.000     2   4.9766\n",
      " : 94 paths kept,  115 examined.\n",
      " : Feff done.  Have a nice day.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root] INFO : Processed file: 1 rh4co000000\n",
      "[root] INFO : Processed file: 2 rh4co000001\n",
      "[root] INFO : Processed file: 3 rh4co000002\n",
      "[root] INFO : Processed file: 4 rh4co000003\n",
      "[root] INFO : Processed file: 5 rh4co000004\n",
      "[root] INFO : Processed file: 6 rh4co000005\n",
      "[root] INFO : Processed file: 7 rh4co000006\n",
      "[root] INFO : Processed file: 8 rh4co000007\n",
      "[root] INFO : Processed file: 9 rh4co000008\n",
      "[root] INFO : Processed file: 10 rh4co000009\n",
      "[root] INFO : Processed file: 11 rh4co000010\n",
      "[root] INFO : Processed file: 12 rh4co000011\n",
      "[root] INFO : Processed file: 13 rh4co000012\n",
      "[root] INFO : Processed file: 14 rh4co000013\n",
      "[root] INFO : Processed file: 15 rh4co000014\n",
      "[root] INFO : Processed file: 16 rh4co000015\n",
      "[root] INFO : Processed file: 17 rh4co000016\n",
      "[root] INFO : Processed file: 18 rh4co000017\n",
      "[root] INFO : Processed file: 19 rh4co000018\n",
      "[root] INFO : Processed file: 20 rh4co000019\n",
      "[root] INFO : Processed file: 21 rh4co000020\n",
      "[root] INFO : Processed file: 22 rh4co000021\n",
      "[root] INFO : Processed file: 23 rh4co000022\n",
      "[root] INFO : Processed file: 24 rh4co000023\n",
      "[root] INFO : Processed file: 25 rh4co000024\n",
      "[root] INFO : Processed file: 26 rh4co000025\n",
      "[root] INFO : Processed file: 27 rh4co000026\n",
      "[root] INFO : Processed file: 28 rh4co000027\n",
      "[root] INFO : Processed file: 29 rh4co000028\n",
      "[root] INFO : Processed file: 30 rh4co000029\n",
      "[root] INFO : Processed file: 31 rh4co000030\n",
      "[root] INFO : Processed file: 32 rh4co000031\n",
      "[root] INFO : Processed file: 33 rh4co000032\n",
      "[root] INFO : Processed file: 34 rh4co000033\n",
      "[root] INFO : Processed file: 35 rh4co000034\n",
      "[root] INFO : Processed file: 36 rh4co000035\n",
      "[root] INFO : Processed file: 37 rh4co000036\n",
      "[root] INFO : Processed file: 38 rh4co000037\n",
      "[root] INFO : Processed file: 39 rh4co000038\n",
      "[root] INFO : Processed file: 40 rh4co000039\n",
      "[root] INFO : Processed file: 41 rh4co000040\n",
      "[root] INFO : Processed file: 42 rh4co000041\n",
      "[root] INFO : Processed file: 43 rh4co000042\n",
      "[root] INFO : Processed file: 44 rh4co000043\n",
      "[root] INFO : Processed file: 45 rh4co000044\n",
      "[root] INFO : Processed file: 46 rh4co000045\n",
      "[root] INFO : Processed file: 47 rh4co000046\n",
      "[root] INFO : Processed file: 48 rh4co000047\n",
      "[root] INFO : Processed file: 49 rh4co000048\n",
      "[root] INFO : Processed file: 50 rh4co000049\n",
      "[root] INFO : Processed file: 51 rh4co000050\n",
      "[root] INFO : Processed file: 52 rh4co000051\n",
      "[root] INFO : Processed file: 53 rh4co000052\n",
      "[root] INFO : Processed file: 54 rh4co000053\n",
      "[root] INFO : Processed file: 55 rh4co000054\n",
      "[root] INFO : Processed file: 56 rh4co000055\n",
      "[root] INFO : Processed file: 57 rh4co000056\n",
      "[root] INFO : Processed file: 58 rh4co000057\n",
      "[root] INFO : Processed file: 59 rh4co000058\n",
      "[root] INFO : Processed file: 60 rh4co000059\n",
      "[root] INFO : Processed file: 61 rh4co000060\n",
      "[root] INFO : Processed file: 62 rh4co000061\n",
      "[root] INFO : Processed file: 63 rh4co000062\n",
      "[root] INFO : Processed file: 64 rh4co000063\n",
      "[root] INFO : Processed file: 65 rh4co000064\n",
      "[root] INFO : Processed file: 66 rh4co000065\n",
      "[root] INFO : Processed file: 67 rh4co000066\n",
      "[root] INFO : Processed file: 68 rh4co000067\n",
      "[root] INFO : Processed file: 69 rh4co000068\n",
      "[root] INFO : Processed file: 70 rh4co000069\n",
      "[root] INFO : Processed file: 71 rh4co000070\n",
      "[root] INFO : Processed file: 72 rh4co000071\n",
      "[root] INFO : Processed file: 73 rh4co000072\n",
      "[root] INFO : Processed file: 74 rh4co000073\n",
      "[root] INFO : Processed file: 75 rh4co000074\n",
      "[root] INFO : Processed file: 76 rh4co000075\n",
      "[root] INFO : Processed file: 77 rh4co000076\n",
      "[root] INFO : Processed file: 78 rh4co000077\n",
      "[root] INFO : Processed file: 79 rh4co000078\n",
      "[root] INFO : Processed file: 80 rh4co000079\n",
      "[root] INFO : Processed file: 81 rh4co000080\n",
      "[root] INFO : Processed file: 82 rh4co000081\n",
      "[root] INFO : Processed file: 83 rh4co000082\n",
      "[root] INFO : Processed file: 84 rh4co000083\n",
      "[root] INFO : Processed file: 85 rh4co000084\n",
      "[root] INFO : Processed file: 86 rh4co000085\n",
      "[root] INFO : Processed file: 87 rh4co000086\n",
      "[root] INFO : Processed file: 88 rh4co000087\n",
      "[root] INFO : Processed file: 89 rh4co000088\n",
      "[root] INFO : Processed file: 90 rh4co000089\n",
      "[root] INFO : Processed file: 91 rh4co000090\n",
      "[root] INFO : Processed file: 92 rh4co000091\n",
      "[root] INFO : Processed file: 93 rh4co000092\n",
      "[root] INFO : Processed file: 94 rh4co000093\n",
      "[root] INFO : Processed file: 95 rh4co000094\n",
      "[root] INFO : Processed file: 96 rh4co000095\n",
      "[root] INFO : Processed file: 97 rh4co000096\n",
      "[root] INFO : Processed file: 98 rh4co000097\n",
      "[root] INFO : Processed file: 99 rh4co000098\n",
      "[root] INFO : Processed file: 100 rh4co000099\n",
      "[root] INFO : Processed file: 101 rh4co000100\n",
      "[root] INFO : Processed file: 102 rh4co000101\n",
      "[root] INFO : Processed file: 103 rh4co000102\n",
      "[root] INFO : Processed file: 104 rh4co000103\n",
      "[root] INFO : Processed file: 105 rh4co000104\n",
      "[root] INFO : Processed file: 106 rh4co000105\n",
      "[root] INFO : Processed file: 107 rh4co000106\n",
      "[root] INFO : Processed file: 108 rh4co000107\n",
      "[root] INFO : Processed file: 109 rh4co000108\n",
      "[root] INFO : Processed file: 110 rh4co000109\n",
      "[root] INFO : Processed file: 111 rh4co000110\n",
      "[root] INFO : Processed file: 112 rh4co000111\n",
      "[root] INFO : Processed file: 113 rh4co000112\n",
      "[root] INFO : Processed file: 114 rh4co000113\n",
      "[root] INFO : Processed file: 115 rh4co000114\n",
      "[root] INFO : Processed file: 116 rh4co000115\n",
      "[root] INFO : Processed file: 117 rh4co000116\n",
      "[root] INFO : Processed file: 118 rh4co000117\n",
      "[root] INFO : Processed file: 119 rh4co000118\n",
      "[root] INFO : Processed file: 120 rh4co000119\n",
      "[root] INFO : Processed file: 121 rh4co000120\n",
      "[root] INFO : Processed file: 122 rh4co000121\n",
      "[root] INFO : Processed file: 123 rh4co000122\n",
      "[root] INFO : Processed file: 124 rh4co000123\n",
      "[root] INFO : Processed file: 125 rh4co000124\n",
      "[root] INFO : Processed file: 126 rh4co000125\n",
      "[root] INFO : Processed file: 127 rh4co000126\n",
      "[root] INFO : Processed file: 128 rh4co000127\n",
      "[root] INFO : Processed file: 129 rh4co000128\n",
      "[root] INFO : Processed file: 130 rh4co000129\n",
      "[root] INFO : Processed file: 131 rh4co000130\n",
      "[root] INFO : Processed file: 132 rh4co000131\n",
      "[root] INFO : Processed file: 133 rh4co000132\n",
      "[root] INFO : Processed file: 134 rh4co000133\n",
      "[root] INFO : Processed file: 135 rh4co000134\n",
      "[root] INFO : Processed file: 136 rh4co000135\n",
      "[root] INFO : Processed file: 137 rh4co000136\n",
      "[root] INFO : Processed file: 138 rh4co000137\n",
      "[root] INFO : Processed file: 139 rh4co000138\n",
      "[root] INFO : Processed file: 140 rh4co000139\n",
      "[root] INFO : Processed file: 141 rh4co000140\n",
      "[root] INFO : Processed file: 142 rh4co000141\n",
      "[root] INFO : Processed file: 143 rh4co000142\n",
      "[root] INFO : Processed file: 144 rh4co000143\n",
      "[root] INFO : Processed file: 145 rh4co000144\n",
      "[root] INFO : Processed file: 146 rh4co000145\n",
      "[root] INFO : Processed file: 147 rh4co000146\n",
      "[root] INFO : Processed file: 148 rh4co000147\n",
      "[root] INFO : Processed file: 149 rh4co000148\n",
      "[root] INFO : Processed file: 150 rh4co000149\n",
      "[root] INFO : Processed file: 151 rh4co000150\n",
      "[root] INFO : Processed file: 152 rh4co000151\n",
      "[root] INFO : Processed file: 153 rh4co000152\n",
      "[root] INFO : Processed file: 154 rh4co000153\n",
      "[root] INFO : Processed file: 155 rh4co000154\n",
      "[root] INFO : Processed file: 156 rh4co000155\n",
      "[root] INFO : Processed file: 157 rh4co000156\n",
      "[root] INFO : Processed file: 158 rh4co000157\n",
      "[root] INFO : Processed file: 159 rh4co000158\n",
      "[root] INFO : Processed file: 160 rh4co000159\n",
      "[root] INFO : Processed file: 161 rh4co000160\n",
      "[root] INFO : Processed file: 162 rh4co000161\n",
      "[root] INFO : Processed file: 163 rh4co000162\n",
      "[root] INFO : Processed file: 164 rh4co000163\n",
      "[root] INFO : Processed file: 165 rh4co000164\n",
      "[root] INFO : Processed file: 166 rh4co000165\n",
      "[root] INFO : Processed file: 167 rh4co000166\n",
      "[root] INFO : Processed file: 168 rh4co000167\n",
      "[root] INFO : Processed file: 169 rh4co000168\n",
      "[root] INFO : Processed file: 170 rh4co000169\n",
      "[root] INFO : Processed file: 171 rh4co000170\n",
      "[root] INFO : Processed file: 172 rh4co000171\n",
      "[root] INFO : Processed file: 173 rh4co000172\n",
      "[root] INFO : Processed file: 174 rh4co000173\n",
      "[root] INFO : Processed file: 175 rh4co000174\n",
      "[root] INFO : Processed file: 176 rh4co000175\n",
      "[root] INFO : Processed file: 177 rh4co000176\n",
      "[root] INFO : Processed file: 178 rh4co000177\n",
      "[root] INFO : Processed file: 179 rh4co000178\n",
      "[root] INFO : Processed file: 180 rh4co000179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root] INFO : Processed file: 181 rh4co000180\n",
      "[root] INFO : Processed file: 182 rh4co000181\n",
      "[root] INFO : Processed file: 183 rh4co000182\n",
      "[root] INFO : Processed file: 184 rh4co000183\n",
      "[root] INFO : Processed file: 185 rh4co000184\n",
      "[root] INFO : Processed file: 186 rh4co000185\n",
      "[root] INFO : Processed file: 187 rh4co000186\n",
      "[root] INFO : Processed file: 188 rh4co000187\n",
      "[root] INFO : Processed file: 189 rh4co000188\n",
      "[root] INFO : Processed file: 190 rh4co000189\n",
      "[root] INFO : Processed file: 191 rh4co000190\n",
      "[root] INFO : Processed file: 192 rh4co000191\n",
      "[root] INFO : Processed file: 193 rh4co000192\n",
      "[root] INFO : Processed file: 194 rh4co000193\n",
      "[root] INFO : Processed file: 195 rh4co000194\n",
      "[root] INFO : Processed file: 196 rh4co000195\n",
      "[root] INFO : Processed file: 197 rh4co000196\n",
      "[root] INFO : Processed file: 198 rh4co000197\n",
      "[root] INFO : Processed file: 199 rh4co000198\n",
      "[root] INFO : Processed file: 200 rh4co000199\n",
      "[root] INFO : Processed file: 201 rh4co000200\n",
      "[root] INFO : Processed file: 202 rh4co000201\n",
      "[root] INFO : Processed file: 203 rh4co000202\n",
      "[root] INFO : Processed file: 204 rh4co000203\n",
      "[root] INFO : Processed file: 205 rh4co000204\n",
      "[root] INFO : Processed file: 206 rh4co000205\n",
      "[root] INFO : Processed file: 207 rh4co000206\n",
      "[root] INFO : Processed file: 208 rh4co000207\n",
      "[root] INFO : Processed file: 209 rh4co000208\n",
      "[root] INFO : Processed file: 210 rh4co000209\n",
      "[root] INFO : Processed file: 211 rh4co000210\n",
      "[root] INFO : Processed file: 212 rh4co000211\n",
      "[root] INFO : Processed file: 213 rh4co000212\n",
      "[root] INFO : Processed file: 214 rh4co000213\n",
      "[root] INFO : Processed file: 215 rh4co000214\n",
      "[root] INFO : Processed file: 216 rh4co000215\n",
      "[root] INFO : Processed file: 217 rh4co000216\n",
      "[root] INFO : Processed file: 218 rh4co000217\n",
      "[root] INFO : Processed file: 219 rh4co000218\n",
      "[root] INFO : Processed file: 220 rh4co000219\n",
      "[root] INFO : Processed file: 221 rh4co000220\n",
      "[root] INFO : Processed file: 222 rh4co000221\n",
      "[root] INFO : Processed file: 223 rh4co000222\n",
      "[root] INFO : Processed file: 224 rh4co000223\n",
      "[root] INFO : Processed file: 225 rh4co000224\n",
      "[root] INFO : Processed file: 226 rh4co000225\n",
      "[root] INFO : Processed file: 227 rh4co000226\n",
      "[root] INFO : Processed file: 228 rh4co000227\n",
      "[root] INFO : Processed file: 229 rh4co000228\n",
      "[root] INFO : Processed file: 230 rh4co000229\n",
      "[root] INFO : Processed file: 231 rh4co000230\n",
      "[root] INFO : Processed file: 232 rh4co000231\n",
      "[root] INFO : Processed file: 233 rh4co000232\n",
      "[root] INFO : Processed file: 234 rh4co000233\n",
      "[root] INFO : Processed file: 235 rh4co000234\n",
      "[root] INFO : Processed file: 236 rh4co000235\n",
      "[root] INFO : Processed file: 237 rh4co000236\n",
      "[root] INFO : Processed file: 238 rh4co000237\n",
      "[root] INFO : Processed file: 239 rh4co000238\n",
      "[root] INFO : Processed file: 240 rh4co000239\n",
      "[root] INFO : Processed file: 241 rh4co000240\n",
      "[root] INFO : Processed file: 242 rh4co000241\n",
      "[root] INFO : Processed file: 243 rh4co000242\n",
      "[root] INFO : Processed file: 244 rh4co000243\n",
      "[root] INFO : Processed file: 245 rh4co000244\n",
      "[root] INFO : Processed file: 246 rh4co000245\n",
      "[root] INFO : Processed file: 247 rh4co000246\n",
      "[root] INFO : Processed file: 248 rh4co000247\n",
      "[root] INFO : Processed file: 249 rh4co000248\n",
      "[root] INFO : Processed file: 250 rh4co000249\n",
      "[root] INFO : Processed file: 251 rh4co000250\n",
      "[root] INFO : Processed file: 252 rh4co000251\n",
      "[root] INFO : Processed file: 253 rh4co000252\n",
      "[root] INFO : Processed file: 254 rh4co000253\n",
      "[root] INFO : Processed file: 255 rh4co000254\n",
      "[root] INFO : Processed file: 256 rh4co000255\n",
      "[root] INFO : Processed file: 257 rh4co000256\n",
      "[root] INFO : Processed file: 258 rh4co000257\n",
      "[root] INFO : Processed file: 259 rh4co000258\n",
      "[root] INFO : Processed file: 260 rh4co000259\n",
      "[root] INFO : Processed file: 261 rh4co000260\n",
      "[root] INFO : Processed file: 262 rh4co000261\n",
      "[root] INFO : Processed file: 263 rh4co000262\n",
      "[root] INFO : Processed file: 264 rh4co000263\n",
      "[root] INFO : Processed file: 265 rh4co000264\n",
      "[root] INFO : Processed file: 266 rh4co000265\n",
      "[root] INFO : Processed file: 267 rh4co000266\n",
      "[root] INFO : Processed file: 268 rh4co000267\n",
      "[root] INFO : Processed file: 269 rh4co000268\n",
      "[root] INFO : Processed file: 270 rh4co000269\n",
      "[root] INFO : Processed file: 271 rh4co000270\n",
      "[root] INFO : Processed file: 272 rh4co000271\n",
      "[root] INFO : Processed file: 273 rh4co000272\n",
      "[root] INFO : Processed file: 274 rh4co000273\n",
      "[root] INFO : Processed file: 275 rh4co000274\n",
      "[root] INFO : Processed file: 276 rh4co000275\n",
      "[root] INFO : Processed file: 277 rh4co000276\n",
      "[root] INFO : Processed file: 278 rh4co000277\n",
      "[root] INFO : Processed file: 279 rh4co000278\n",
      "[root] INFO : Processed file: 280 rh4co000279\n",
      "[root] INFO : Processed file: 281 rh4co000280\n",
      "[root] INFO : Processed file: 282 rh4co000281\n",
      "[root] INFO : Processed file: 283 rh4co000282\n",
      "[root] INFO : Processed file: 284 rh4co000283\n",
      "[root] INFO : Processed file: 285 rh4co000284\n",
      "[root] INFO : Processed file: 286 rh4co000285\n",
      "[root] INFO : Processed file: 287 rh4co000286\n",
      "[root] INFO : Processed file: 288 rh4co000287\n",
      "[root] INFO : Processed file: 289 rh4co000288\n",
      "[root] INFO : Processed file: 290 rh4co000289\n",
      "[root] INFO : Processed file: 291 rh4co000290\n",
      "[root] INFO : Processed file: 292 rh4co000291\n",
      "[root] INFO : Processed file: 293 rh4co000292\n",
      "[root] INFO : Processed file: 294 rh4co000293\n",
      "[root] INFO : Processed file: 295 rh4co000294\n",
      "[root] INFO : Processed file: 296 rh4co000295\n",
      "[root] INFO : Processed file: 297 rh4co000296\n",
      "[root] INFO : Processed file: 298 rh4co000297\n",
      "[root] INFO : Processed file: 299 rh4co000298\n",
      "[root] INFO : Processed file: 300 rh4co000299\n",
      "[root] INFO : Processed file: 301 rh4co000300\n",
      "[root] INFO : Processed file: 302 rh4co000301\n",
      "[root] INFO : Processed file: 303 rh4co000302\n",
      "[root] INFO : Processed file: 304 rh4co000303\n",
      "[root] INFO : Processed file: 305 rh4co000304\n",
      "[root] INFO : Processed file: 306 rh4co000305\n",
      "[root] INFO : Processed file: 307 rh4co000306\n",
      "[root] INFO : Processed file: 308 rh4co000307\n",
      "[root] INFO : Processed file: 309 rh4co000308\n",
      "[root] INFO : Processed file: 310 rh4co000309\n",
      "[root] INFO : Processed file: 311 rh4co000310\n",
      "[root] INFO : Processed file: 312 rh4co000311\n",
      "[root] INFO : Processed file: 313 rh4co000312\n",
      "[root] INFO : Processed file: 314 rh4co000313\n",
      "[root] INFO : Processed file: 315 rh4co000314\n",
      "[root] INFO : Processed file: 316 rh4co000315\n",
      "[root] INFO : Processed file: 317 rh4co000316\n",
      "[root] INFO : Processed file: 318 rh4co000317\n",
      "[root] INFO : Processed file: 319 rh4co000318\n",
      "[root] INFO : Processed file: 320 rh4co000319\n",
      "[root] INFO : Processed file: 321 rh4co000320\n",
      "[root] INFO : Processed file: 322 rh4co000321\n",
      "[root] INFO : Processed file: 323 rh4co000322\n",
      "[root] INFO : Processed file: 324 rh4co000323\n",
      "[root] INFO : Processed file: 325 rh4co000324\n",
      "[root] INFO : Processed file: 326 rh4co000325\n",
      "[root] INFO : Processed file: 327 rh4co000326\n",
      "[root] INFO : Processed file: 328 rh4co000327\n",
      "[root] INFO : Processed file: 329 rh4co000328\n",
      "[root] INFO : Processed file: 330 rh4co000329\n",
      "[root] INFO : Processed file: 331 rh4co000330\n",
      "[root] INFO : Processed file: 332 rh4co000331\n",
      "[root] INFO : Processed file: 333 rh4co000332\n",
      "[root] INFO : Processed file: 334 rh4co000333\n",
      "[root] INFO : Processed file: 335 rh4co000334\n",
      "[root] INFO : Processed file: 336 rh4co000335\n",
      "[root] INFO : Processed file: 337 rh4co000336\n",
      "[root] INFO : Processed file: 338 rh4co000337\n",
      "[root] INFO : Processed file: 339 rh4co000338\n",
      "[root] INFO : Processed file: 340 rh4co000339\n",
      "[root] INFO : Processed file: 341 rh4co000340\n",
      "[root] INFO : Processed file: 342 rh4co000341\n",
      "[root] INFO : Processed file: 343 rh4co000342\n",
      "[root] INFO : Processed file: 344 rh4co000343\n",
      "[root] INFO : Processed file: 345 rh4co000344\n",
      "[root] INFO : Processed file: 346 rh4co000345\n",
      "[root] INFO : Processed file: 347 rh4co000346\n",
      "[root] INFO : Processed file: 348 rh4co000347\n",
      "[root] INFO : Processed file: 349 rh4co000348\n",
      "[root] INFO : Processed file: 350 rh4co000349\n",
      "[root] INFO : Processed file: 351 rh4co000350\n",
      "[root] INFO : Processed file: 352 rh4co000351\n",
      "[root] INFO : Processed file: 353 rh4co000352\n",
      "[root] INFO : Processed file: 354 rh4co000353\n",
      "[root] INFO : Processed file: 355 rh4co000354\n",
      "[root] INFO : Processed file: 356 rh4co000355\n",
      "[root] INFO : Processed file: 357 rh4co000356\n",
      "[root] INFO : Processed file: 358 rh4co000357\n",
      "[root] INFO : Processed file: 359 rh4co000358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root] INFO : Processed file: 360 rh4co000359\n",
      "[root] INFO : Processed file: 361 rh4co000360\n",
      "[root] INFO : Processed file: 362 rh4co000361\n",
      "[root] INFO : Processed file: 363 rh4co000362\n",
      "[root] INFO : Processed file: 364 rh4co000363\n",
      "[root] INFO : Processed file: 365 rh4co000364\n",
      "[root] INFO : Processed file: 366 rh4co000365\n",
      "[root] INFO : Processed file: 367 rh4co000366\n",
      "[root] INFO : Processed file: 368 rh4co000367\n",
      "[root] INFO : Processed file: 369 rh4co000368\n",
      "[root] INFO : Processed file: 370 rh4co000369\n",
      "[root] INFO : Processed file: 371 rh4co000370\n",
      "[root] INFO : Processed file: 372 rh4co000371\n",
      "[root] INFO : Processed file: 373 rh4co000372\n",
      "[root] INFO : Processed file: 374 rh4co000373\n",
      "[root] INFO : Processed file: 375 rh4co000374\n",
      "[root] INFO : Processed file: 376 rh4co000375\n",
      "[root] INFO : Processed file: 377 rh4co000376\n",
      "[root] INFO : Processed file: 378 rh4co000377\n",
      "[root] INFO : Processed file: 379 rh4co000378\n",
      "[root] INFO : Processed file: 380 rh4co000379\n",
      "[root] INFO : Processed file: 381 rh4co000380\n",
      "[root] INFO : Processed file: 382 rh4co000381\n",
      "[root] INFO : Processed file: 383 rh4co000382\n",
      "[root] INFO : Processed file: 384 rh4co000383\n",
      "[root] INFO : Processed file: 385 rh4co000384\n",
      "[root] INFO : Processed file: 386 rh4co000385\n",
      "[root] INFO : Processed file: 387 rh4co000386\n",
      "[root] INFO : Processed file: 388 rh4co000387\n",
      "[root] INFO : Processed file: 389 rh4co000388\n",
      "[root] INFO : Processed file: 390 rh4co000389\n",
      "[root] INFO : Processed file: 391 rh4co000390\n",
      "[root] INFO : Processed file: 392 rh4co000391\n",
      "[root] INFO : Processed file: 393 rh4co000392\n",
      "[root] INFO : Processed file: 394 rh4co000393\n",
      "[root] INFO : Processed file: 395 rh4co000394\n",
      "[root] INFO : Processed file: 396 rh4co000395\n",
      "[root] INFO : Processed file: 397 rh4co000396\n",
      "[root] INFO : Processed file: 398 rh4co000397\n",
      "[root] INFO : Processed file: 399 rh4co000398\n",
      "[root] INFO : Processed file: 400 rh4co000399\n",
      "[root] INFO : Finished processing\n"
     ]
    }
   ],
   "source": [
    "# create the path for storing results\n",
    "base_path = Path(\"./\" , f_prefix+\"_fit\")\n",
    "Path(base_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_file = Path(\"./\",base_path,\"process.log\")\n",
    "print(log_file)\n",
    "# set path for log\n",
    "set_logger(log_file)\n",
    "\n",
    "# get the list of files to process\n",
    "source_path = Path(data_path)\n",
    "files_list = get_files_list(source_path, file_pattern)\n",
    "xas_data = {}\n",
    "\n",
    "logging.info(\"Started processing\")\n",
    "logging.info(\"Input variables:\")\n",
    "logging.info(\"\\tdata_path    = \" + data_path)\n",
    "logging.info(\"\\tfile_pattern = \" + file_pattern)\n",
    "logging.info(\"\\tf_prefix     = \" + f_prefix)\n",
    "logging.info(\"\\tcrystal_file = \" + crystal_file)\n",
    "logging.info(\"\\ttop_count    = \" + str(top_count))\n",
    "\n",
    "# read the gds parameters from input file\n",
    "gds = read_gds(gds_parms_f)\n",
    "logging.info(\"GDS Parameters read OK\")\n",
    "\n",
    "# run feff on crystal file to generate scattering paths\n",
    "run_feff(crystal_file)\n",
    "logging.info(\"Completed FEFF\")\n",
    "# read the selected paths list to access relevant paths \n",
    "# generated from FEFF\n",
    "#selected_paths = read_paths(sel_paths_f)\n",
    "\n",
    "# counter for break\n",
    "i_count = 0\n",
    "for a_file in files_list:\n",
    "    project_name = a_file.name\n",
    "    data_prj = read_athena(a_file)\n",
    "    group_keys = list(data_prj._athena_groups.keys())\n",
    "    data_group = extract_athenagroup(data_prj._athena_groups[group_keys[0]])\n",
    "    \n",
    "    \n",
    "    if show_graph:    \n",
    "        # plot normalised mu on energy\n",
    "        # plot mu vs flat normalised mu for selected groups\n",
    "        #plt, data_group = plot_Nxmu_E(data_prj, group_keys, group_names)\n",
    "        plt = plot_normalised(data_group)\n",
    "        plt.show()\n",
    "    \n",
    "        \n",
    "    i_count +=1\n",
    "    \n",
    "    logging.info(\"Processed file: \"+ str(i_count) +\" \" + group_keys[0])\n",
    "    \n",
    "    if i_count == top_count:\n",
    "        break\n",
    "       \n",
    "logging.info(\"Finished processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C12O12Rh4.cif\n",
      "..\\cif_files\n",
      "Passed!\n",
      " : ======== running Feff module C:\\Users\\scman1\\Anaconda3\\envs\\python36\\lib\\site-packages\\larch\\bin\\win32\\feff6l.exe ========\n",
      " : Feff 6L.02\n",
      " : Rh4 (C O)12\n",
      " : Wei, C.H.\n",
      " : Structural analyses of tetracobalt dodecacarbonyl and tetrarhodium\n",
      " : dodecacarbonyl. crystallographic treatments of a disordered structure and\n",
      " : a twinned composite\n",
      " : Calculating potentials and phases...\n",
      " : free atom potential and density for atom type    0\n",
      " : free atom potential and density for atom type    1\n",
      " : free atom potential and density for atom type    2\n",
      " : free atom potential and density for atom type    3\n",
      " : overlapped potential and density for unique potential    0\n",
      " : overlapped potential and density for unique potential    1\n",
      " : overlapped potential and density for unique potential    2\n",
      " : overlapped potential and density for unique potential    3\n",
      " : muffin tin radii and interstitial parameters\n",
      " : phase shifts for unique potential    0\n",
      " : phase shifts for unique potential    1\n",
      " : phase shifts for unique potential    2\n",
      " : phase shifts for unique potential    3\n",
      " : Preparing plane wave scattering amplitudes...\n",
      " : nncrit in prcrit       9\n",
      " : Searching for paths...\n",
      " : Rmax  5.0000  keep and heap limits   0.0000000   0.0000000\n",
      " : Preparing neighbor table\n",
      " : nfound  nheap  nheapx  nsc    r\n",
      " : Paths found      403   (nheapx, nbx      99   8)\n",
      " : Eliminating path degeneracies...\n",
      " : Plane wave chi amplitude filter   2.50%\n",
      " : Unique paths    115,  total paths     177\n",
      " : Calculating EXAFS parameters...\n",
      " : Curved wave chi amplitude ratio   4.00%\n",
      " : Discard feff.dat for paths with cw ratio <   2.67%\n",
      " : path  cw ratio     deg    nleg  reff\n",
      " : 1   100.000     1.000     2   1.9149\n",
      " : 2    91.295     1.000     2   1.9854\n",
      " : 3    83.637     1.000     2   2.0550\n",
      " : 4    79.929     1.000     2   2.0917\n",
      " : 5    89.367     1.000     2   2.7050\n",
      " : 6    86.941     1.000     2   2.7337\n",
      " : 7    81.971     1.000     2   2.7957\n",
      " : 8    37.159     1.000     2   3.0163\n",
      " : 9   100.000     2.000     3   3.0167\n",
      " : 10    76.145     1.000     4   3.0170\n",
      " : 11    32.597     1.000     2   3.0220\n",
      " : 12    29.179     1.000     2   3.1366\n",
      " : 13    25.921     2.000     3   3.1653\n",
      " : 14    27.950     1.000     2   3.1819\n",
      " : 15    58.926     2.000     3   3.2132\n",
      " : 16    32.939     1.000     4   3.2445\n",
      " : 17    21.901     2.000     3   3.2875\n",
      " : 18     7.728     1.000     4   3.3087\n",
      " : 19     5.677     2.000     3   3.3214\n",
      " : 20     5.763     2.000     3   3.3260\n",
      " : 21     6.231     1.000     4   3.4384\n",
      " : 22     2.425     2.000     3   3.4558 neglected\n",
      " : 23    16.232     1.000     2   3.5372\n",
      " : 24     3.728     2.000     3   3.5425\n",
      " : 25    15.819     1.000     2   3.5671\n",
      " : 26     3.312     2.000     3   3.5939\n",
      " : 27     4.223     2.000     3   3.6181\n",
      " : 28     2.572     2.000     3   3.6489 neglected\n",
      " : 29    14.227     1.000     2   3.6920\n",
      " : 30    11.517     2.000     3   3.8220\n",
      " : 31     4.455     1.000     4   3.8297\n",
      " : 32    12.091     1.000     2   3.8888\n",
      " : 33    15.264     2.000     4   3.9003\n",
      " : 34    14.184     1.000     2   3.9535\n",
      " : 35     3.684     1.000     4   3.9708\n",
      " : 36    10.240     1.000     4   3.9783\n",
      " : 37    32.052     2.000     5   3.9787\n",
      " : 38    25.210     1.000     6   3.9790\n",
      " : 39    11.124     1.000     2   3.9920\n",
      " : 40    13.715     1.000     2   3.9945\n",
      " : 41     8.332     2.000     3   4.0838\n",
      " : 42     3.048     2.000     3   4.0890\n",
      " : 43     3.260     2.000     3   4.1013\n",
      " : 44     3.073     1.000     4   4.1100\n",
      " : 45     7.914     2.000     3   4.1209\n",
      " : 46     3.206     2.000     3   4.1223\n",
      " : 47     2.799     1.000     4   4.1834\n",
      " : 48     7.874     2.000     3   4.1912\n",
      " : 49     1.425     2.000     3   4.2145 neglected\n",
      " : 50     8.949     1.000     2   4.2696\n",
      " : 51    10.748     1.000     2   4.3002\n",
      " : 52    10.714     1.000     2   4.3043\n",
      " : 53    10.621     1.000     2   4.3155\n",
      " : 54     7.182     2.000     3   4.3251\n",
      " : 55     5.235     1.000     4   4.3347\n",
      " : 56     1.577     2.000     3   4.3415 neglected\n",
      " : 57     3.558     1.000     4   4.3452\n",
      " : 58     8.350     1.000     2   4.3603\n",
      " : 59    12.514     2.000     5   4.3660\n",
      " : 60    16.294     2.000     3   4.3787\n",
      " : 61     1.866     2.000     3   4.3964 neglected\n",
      " : 62     7.648     1.000     6   4.3972\n",
      " : 63     6.643     2.000     3   4.4001\n",
      " : 64    13.337     2.000     3   4.4055\n",
      " : 65     2.179     2.000     4   4.4289 neglected\n",
      " : 66     5.926     2.000     3   4.4541\n",
      " : 67     1.929     2.000     4   4.4805 neglected\n",
      " : 68     9.336     1.000     2   4.4832\n",
      " : 69     7.222     1.000     4   4.4878\n",
      " : 70     4.653     2.000     5   4.4886\n",
      " : 71    12.751     2.000     3   4.4927\n",
      " : 72     5.932     1.000     4   4.4955\n",
      " : 73     2.547     2.000     5   4.5045 neglected\n",
      " : 74     2.232     2.000     4   4.5397 neglected\n",
      " : 75     2.262     2.000     5   4.5559 neglected\n",
      " : 76     1.888     2.000     3   4.5571 neglected\n",
      " : 77     1.544     2.000     3   4.5599 neglected\n",
      " : 78     3.284     2.000     3   4.5615\n",
      " : 79     2.902     2.000     3   4.5732\n",
      " : 80     3.436     2.000     3   4.6064\n",
      " : 81     4.653     1.000     4   4.6251\n",
      " : 82     6.854     2.000     3   4.6311\n",
      " : 83     6.727     1.000     2   4.6507\n",
      " : 84     8.204     1.000     2   4.6551\n",
      " : 85     2.780     2.000     3   4.6567\n",
      " : 86     2.354     1.000     4   4.6601 neglected\n",
      " : 87     3.264     2.000     3   4.6645\n",
      " : 88     6.619     1.000     2   4.6730\n",
      " : 89     3.068     2.000     4   4.6823\n",
      " : 90     1.803     2.000     4   4.7108 neglected\n",
      " : 91    18.587     2.000     3   4.7211\n",
      " : 92    13.756     1.000     4   4.7692\n",
      " : 93     2.182     1.000     4   4.7791 neglected\n",
      " : 94     6.951     2.000     3   4.7831\n",
      " : 95     6.102     2.000     3   4.7842\n",
      " : 96     2.095     2.000     4   4.7887 neglected\n",
      " : 97     2.457     2.000     4   4.7891 neglected\n",
      " : 98    10.393     2.000     4   4.7967\n",
      " : 99     2.759     2.000     5   4.8110\n",
      " : 100     0.984     2.000     3   4.8201 neglected\n",
      " : 101     1.949     2.000     4   4.8507 neglected\n",
      " : 102     7.049     1.000     2   4.8622\n",
      " : 103     2.435     2.000     3   4.8868 neglected\n",
      " : 104     6.792     1.000     2   4.9136\n",
      " : 105     5.690     2.000     3   4.9161\n",
      " : 106     2.369     1.000     4   4.9155 neglected\n",
      " : 107    11.074     2.000     3   4.9167\n",
      " : 108    10.137     2.000     4   4.9368\n",
      " : 109     5.170     1.000     6   4.9403\n",
      " : 110    16.546     2.000     7   4.9407\n",
      " : 111    13.280     1.000     8   4.9410\n",
      " : 112     3.562     2.000     3   4.9549\n",
      " : 113     5.689     2.000     4   4.9593\n",
      " : 114     5.096     1.000     4   4.9712\n",
      " : 115     6.492     1.000     2   4.9766\n",
      " : 94 paths kept,  115 examined.\n",
      " : Feff done.  Have a nice day.\n"
     ]
    }
   ],
   "source": [
    "var =\"..\\\\cif_files\\\\C12O12Rh4.cif\"; \"FeS2.inp\"\n",
    "crystal_f = Path(var)\n",
    "print(crystal_f.name)\n",
    "print(crystal_f.parent)\n",
    "prefix = \"fes2\"\n",
    "feff_dir = var[:-4]+\"_feff\"\n",
    "feff_dir \n",
    "run_feff(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select paths from FEFF\n",
    "To select a path change the value of the select column to 1 in the table displayed after running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'select_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-38f3e492407c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpathsheet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[0mpath_sheet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'FeS2.inp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_sheet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'select_paths' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import ipysheet\n",
    "\n",
    "def show_paths(var = \"FeS2.inp\"):\n",
    "    crystal_f = Path(var)\n",
    "    feff_dir = crystal_f.name[:-4]+\"_feff\"\n",
    "    feff_inp = crystal_f.name[:-4]+\"_feff.inp\"\n",
    "    feff_files = \"files.dat\"\n",
    "    input_file = Path(feff_dir, feff_files)\n",
    "    #check if feff dir exists\n",
    "    if input_file.parent.exists() and input_file.exists():\n",
    "        logging.info(str(input_file.parent) + \" path and \"+ str(input_file)+ \" found\")\n",
    "    else:\n",
    "        logging.info(str(input_file.parent) + \" path not found, run feff before running select paths\")\n",
    "        return False\n",
    "    count = 0\n",
    "    # the .dat data is stored in fixed width strings \n",
    "    field_widths = [[0,13],[14,21],[22,31],[32,41],[42,48],[49,61]]\n",
    "    is_meta = True\n",
    "    data_headers = []\n",
    "    path_count = 0\n",
    "    paths_data = []\n",
    "    logging.info(\"Reading from: \"+ str(input_file))\n",
    "    with open(input_file) as datfile:\n",
    "        dat_lines = datfile.readlines()\n",
    "        for a_line in dat_lines:\n",
    "            count += 1\n",
    "            if re.match('-*', a_line.strip()).group(0)!= '':\n",
    "                is_meta = False\n",
    "                logging.info(\"{}: {}\".format(count, a_line.strip()))\n",
    "            elif is_meta:\n",
    "                logging.info(\"{}: {}\".format(count, a_line.strip()))\n",
    "            elif data_headers == []:\n",
    "                data_headers = [a_line[s:e].strip().replace(' ','_') for s,e in field_widths]\n",
    "                logging.info(\"headers:\"+ str(data_headers))\n",
    "                data_headers.append('select')\n",
    "                paths_data.append(data_headers)\n",
    "            else:\n",
    "                path_count += 1\n",
    "                data_values = [a_line[s:e].strip() for s,e in field_widths]\n",
    "                data_values.append(0)\n",
    "                data_values[0] = feff_dir+\"/\"+data_values[0]\n",
    "                paths_data.append(data_values)\n",
    "    # use data to populate spreadsheet\n",
    "    \n",
    "    pathsheet = ipysheet.sheet(rows=path_count+1, columns=7)\n",
    "    ipysheet.cell_range(paths_data)\n",
    "    return pathsheet\n",
    "\n",
    "path_sheet = select_paths('FeS2.inp')\n",
    "display(path_sheet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters for each selected path\n",
    "To define the parameters enter the corresponding values in the table that appears after running the code in the cell below this message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_sheet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-8654accb1b35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_sheet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mipysheet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_sheet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselected\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_sheet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"A\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_sheet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"G\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mselected\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'1'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path_sheet' is not defined"
     ]
    }
   ],
   "source": [
    "df_sheet = ipysheet.to_dataframe(path_sheet)\n",
    "files = []\n",
    "for f_name, selected in zip(df_sheet[\"A\"], df_sheet[\"G\"]):\n",
    "    if selected == '1':\n",
    "        files.append(f_name)    \n",
    "        \n",
    "sheet = ipysheet.sheet(rows=5, columns=4)\n",
    "row = ipysheet.row(0, [0, 1, 2, 3], background_color=\"red\")\n",
    "column = ipysheet.column(1, [\"a\", \"b\", \"c\", \"d\"], row_start=1, background_color=\"green\")\n",
    "cells = ipysheet.cell_range([[\"hi\", \"ola\"], [\"ciao\", \"bonjour\"], [\"hallo\", \"guten tag\"]],\n",
    "                            row_start=1, column_start=2, background_color=\"yellow\")\n",
    "sel_paths_data = [[0 for col in range(6)] for row in range(4)]\n",
    "sel_paths_data[:0]=[['file','label','s02','e0','sigma2','deltar']]\n",
    "ps_row = 1\n",
    "for a_name in files:\n",
    "    sel_paths_data[ps_row][0] = a_name\n",
    "    ps_row += 1\n",
    "\n",
    "sp_sheet = ipysheet.sheet(rows=len(files)+1, columns=6)\n",
    "ipysheet.cell_range(sel_paths_data)\n",
    "display(sp_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sp_sheet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-aa2afe9ab273>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msp_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mipysheet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp_sheet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msp_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msp_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# use data frame to create selected paths list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sp_sheet' is not defined"
     ]
    }
   ],
   "source": [
    "sp_data = ipysheet.to_dataframe(sp_sheet)\n",
    "sp_data\n",
    "sp_data\n",
    "# use data frame to create selected paths list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Paths\n",
    "The selection of scattering paths is shown in the following code. Each scattering path is loaded while setting the amplitude, $\\Delta E_0$, $\\Delta R$ and $\\sigma^2$ parameters using the GDS parameters defined previously.\n",
    "\n",
    "The groups are then added to a list of paths to be used for the fit.\n",
    "This task is different from the one in Demeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'FeS2_dmtr.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-63eb97a1d631>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp_s1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_s2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_s3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_fe\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[0mselected_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#sel_paths_f, crystal_file)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[0mselected_paths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-63eb97a1d631>\u001b[0m in \u001b[0;36mread_paths\u001b[1;34m(sel_paths, crystal_file)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_paths\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msel_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'FeS2_dmtr.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrystal_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'FeS2.inp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInterpreter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mcsv_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_csv_no_id_no_head\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msel_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     p_s1 = lp.xafs.FeffPathGroup(filename = 'fes2_feff/feff0001.dat',\n",
      "\u001b[1;32m<ipython-input-22-63eb97a1d631>\u001b[0m in \u001b[0;36mget_csv_no_id_no_head\u001b[1;34m(input_file)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_csv_no_id_no_head\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mcsv_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mrow_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FeS2_dmtr.csv'"
     ]
    }
   ],
   "source": [
    "# import library for managing csv files\n",
    "import csv\n",
    "\n",
    "# getting the data from the demeter csv_file, \n",
    "# with no headers does not work. We need an\n",
    "# alternative for using the FEFF data produced\n",
    "# by Larch\n",
    "\n",
    "def get_csv_no_id_no_head(input_file):\n",
    "    csv_data = {}\n",
    "    with open(input_file, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        row_id = 0\n",
    "        for row in reader:\n",
    "            csv_data[row_id]=row\n",
    "            row_id += 1\n",
    "    return csv_data\n",
    "\n",
    "# select paths\n",
    "# labelling for reference only using Artemis-FEFF given names\n",
    "def read_paths (sel_paths = 'FeS2_dmtr.csv', crystal_file = 'FeS2.inp'):\n",
    "    session = Interpreter()\n",
    "    csv_paths = get_csv_no_id_no_head(sel_paths)\n",
    "    print(csv_paths)\n",
    "    p_s1 = lp.xafs.FeffPathGroup(filename = 'fes2_feff/feff0001.dat',\n",
    "                                  label    = \"S1\",\n",
    "                                  s02      = 'amp',\n",
    "                                  e0       = 'enot',\n",
    "                                  sigma2   = 'ss',\n",
    "                                  deltar   = 'alpha*reff',\n",
    "                                  _larch   = session)\n",
    "    p_s2 = lp.xafs.FeffPathGroup(filename = 'fes2_feff/feff0002.dat',\n",
    "                                  label    = \"S2\",\n",
    "                                  s02      = 'amp',\n",
    "                                  e0       = 'enot',\n",
    "                                  sigma2   = 'ss2',\n",
    "                                  deltar   = 'alpha*reff',\n",
    "                                  _larch   = session)\n",
    "    p_s3 = lp.xafs.FeffPathGroup(filename = 'fes2_feff/feff0003.dat',\n",
    "                                  label    = \"S3\",\n",
    "                                  s02      = 'amp',\n",
    "                                  e0       = 'enot',\n",
    "                                  sigma2   = 'ss3',\n",
    "                                  deltar   = 'alpha*reff',\n",
    "                                  _larch   = session)\n",
    "    p_fe = lp.xafs.FeffPathGroup(filename = 'fes2_feff/feff0004.dat',\n",
    "                                  label    = \"Fe\",\n",
    "                                  s02      = 'amp',\n",
    "                                  e0       = 'enot',\n",
    "                                  sigma2   = 'ssfe',\n",
    "                                  deltar   = 'alpha*reff',\n",
    "                                  _larch   = session)\n",
    "    # return path list\n",
    "    return [p_s1, p_s2, p_s3, p_fe]\n",
    "\n",
    "selected_paths = read_paths()#sel_paths_f, crystal_file)\n",
    "selected_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_paths = 'FeS2_dmtr.csv'\n",
    "csv_paths = get_csv_no_id_no_head(sel_paths)\n",
    "\n",
    "print(csv_paths)\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "data = [[1,2,3],\n",
    "        [4,5,6],\n",
    "        [7,8,9],\n",
    "        ]\n",
    "\n",
    "display(HTML(\n",
    "   '<table><tr>{}</tr></table>'.format(\n",
    "       '</tr><tr>'.join(\n",
    "           '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in data)\n",
    "       )\n",
    "))\n",
    "\n",
    "table = [[\"Sun\",696000,1989100000],\n",
    "         [\"Earth\",6371,5973.6],\n",
    "         [\"Moon\",1737,73.5],\n",
    "         [\"Mars\",3390,641.85]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Fit\n",
    "\n",
    "XAS fitting is performed in three steps:\n",
    "1. Create a Transform group to holds the set of Fourier transform parameters, fitting ranges, and space in which the data and sum of paths are to be compared (R space)\n",
    "2. Create a Dataset group,consistaining of the three components required for fitting(data, paths, and transform group)\n",
    "3. FEFFIT is run with the list of parameters (gds) for the fit, and the dataset or list of datasets groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run fit\n",
    "# create the transform grup (prepare the fit space).\n",
    "trans = lp.xafs.TransformGroup(fitspace='r', kmin=3, kmax=14, kw=2, dk=1, window='hanning', rmin=1.4,\n",
    "                               rmax=3.0, _larch=session)\n",
    "\n",
    "\n",
    "dset = lp.xafs.FeffitDataSet(data=data_group, pathlist=selected_paths, transform=trans, _larch=session)\n",
    "\n",
    "out = lp.xafs.feffit(gds, dset, _larch=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review fit results\n",
    "The results of the fit are stored in the dataset. These can be plotted and printed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(dset.data.r, dset.data.chir_mag, color='b')\n",
    "plt.plot(dset.data.r, dset.data.chir_re, color='b', label='expt.')\n",
    "plt.plot(dset.model.r, dset.model.chir_mag, color='r')\n",
    "plt.plot(dset.model.r, dset.model.chir_re, color='r', label='fit')\n",
    "plt.ylabel(\"Magnitude of Fourier Transform of $k^2 \\cdot \\chi$/$\\mathrm{\\AA}^{-3}$\")\n",
    "plt.xlabel(\"Radial distance/$\\mathrm{\\AA}$\")\n",
    "plt.xlim(0, 5)\n",
    "\n",
    "plt.fill([1.4, 1.4, 3.0, 3.0],[-3, 3, 3, -3], color='g',alpha=0.1)\n",
    "plt.text(2.35, -2.5, 'fit range')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "# Creating the chifit plot from scratch\n",
    "#from larch.wxlib.xafsplots import plot_chifit\n",
    "#plot_chifit(dset, _larch=session)\n",
    "ax1.plot(dset.data.k, dset.data.chi*dset.data.k**2, color='b', label='expt.')\n",
    "ax1.plot(dset.model.k, dset.model.chi*dset.data.k**2 , color='r', label='fit')\n",
    "ax1.set_xlim(0, 15)\n",
    "ax1.set_xlabel(\"$k (\\mathrm{\\AA})^{-1}$\")\n",
    "ax1.set_ylabel(\"$k^2$ $\\chi (k)(\\mathrm{\\AA})^{-2}$\")\n",
    "ax1.fill([3.0, 3.0, 14.0, 14.0],[-3, 3, 3, -3], color='g',alpha=0.1)\n",
    "ax1.text(12.35, -2.5, 'fit range')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(dset.data.r, dset.data.chir_mag, color='b', label='expt.')\n",
    "ax2.plot(dset.model.r, dset.model.chir_mag, color='r', label='fit')\n",
    "ax2.set_xlim(0, 5)\n",
    "ax2.set_xlabel(\"$R(\\mathrm{\\AA})$\")\n",
    "ax2.set_ylabel(\"$|\\chi(R)|(\\mathrm{\\AA}^{-3})$\")\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.fill([1.4, 1.4, 3.0, 3.0],[0, 3, 3, 0], color='g',alpha=0.1)\n",
    "ax2.text(2.35, 2.75, 'fit range')\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lp.xafs.feffit_report(out, _larch=session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(gds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(p_s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to do\n",
    "- for manual task\n",
    " - ~Allow setting parameters interactively (if CSV file does not exist or is missing)~\n",
    " - Move all functions to libraries (hide complexity from researchers)\n",
    " - Test running with textbook example (FeS2)\n",
    " - Test running with sample data (Rh4CO12)\n",
    "- for batch processing\n",
    " - replicate in batch processing (reading of parameters and paths)\n",
    " - test run automatic with textbook example\n",
    " - test run automatic with sample data (Rh4CO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
